import logging
import asyncio
import json
from typing import Dict, Any, List, Optional, AsyncGenerator
from pkg.core.llm import (
    LLMAggregator,
    Manager,
    get_manager,
    ChatCompletionRequest,
    ChatCompletionResponse,
    Message,
    MessageRole,
    GeminiProvider,
    ModelConfig,
    ProviderConfig
)

logger = logging.getLogger(__name__)

def format_messages(messages: List[Message]) -> str:
    """æ ¼å¼åŒ–æ¶ˆæ¯åˆ—è¡¨ï¼Œä½¿æ—¥å¿—æ›´æ˜“è¯»"""
    formatted = []
    for msg in messages:
        formatted.append({
            "role": msg.role,
            "content": msg.content[:100] + "..." if len(msg.content) > 100 else msg.content
        })
    return json.dumps(formatted, indent=2, ensure_ascii=False)

class DemoService:
    """
    DemoæœåŠ¡ - æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨LLMæ¶æ„è°ƒç”¨Geminiæ¨¡å‹
    """
    
    def __init__(self):
        self.manager: Manager = get_manager()
        self.aggregator: Optional[LLMAggregator] = None
        self._initialized = False
    
    async def initialize(self, gemini_config: Dict[str, Any]) -> None:
        """åˆå§‹åŒ–DemoæœåŠ¡"""
        if self._initialized:
            logger.info("DemoService already initialized")
            return
        
        try:
            # åˆ›å»ºGemini Provider
            gemini_provider = GeminiProvider.create_from_config(
                provider_name="gemini",
                models_config=gemini_config.get("models", {})
            )
            
            # è°ƒè¯•ï¼šæ£€æŸ¥providerå†…éƒ¨çŠ¶æ€
            logger.info(f"Provider clients: {list(gemini_provider.clients.keys())}")
            for model_name, client in gemini_provider.clients.items():
                logger.info(f"Model {model_name} client available: {client.is_available()}")
            
            # æ³¨å†ŒProvideråˆ°Manager
            self.manager.register_provider(gemini_provider)
            
            # éªŒè¯æ³¨å†Œæ˜¯å¦æˆåŠŸ
            registered_providers = self.manager.get_all_providers()
            all_models = self.manager.get_all_models()
            
            logger.info(f"Registered providers: {registered_providers}")
            logger.info(f"Available models: {all_models}")
            
            self._initialized = True
            logger.info("DemoService initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize DemoService: {e}")
            raise

    async def simple_chat(
        self, 
        message: str, 
        model: str = "gemini-2.5-flash",
        system_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """ç®€å•èŠå¤©æ¥å£"""
        if not self._initialized:
            raise RuntimeError("DemoService not initialized")
        
        try:
            # å…ˆæ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
            available_models = self.manager.get_all_models()
            logger.info(f"Looking for model {model} in available models: {available_models}")
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = []
            
            # å¦‚æœæœ‰system promptï¼Œæ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨å¼€å¤´
            if system_prompt:
                messages.append(Message(role=MessageRole.SYSTEM, content=system_prompt))
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            messages.append(Message(role=MessageRole.USER, content=message))
            
            # è®°å½•è¯·æ±‚ä¿¡æ¯
            logger.info(f"\n{'='*50}\nğŸ¤– AI Request:\n"
                       f"Model: {model}\n"
                       f"Messages:\n{format_messages(messages)}\n{'='*50}")
            
            # åˆ›å»ºèŠå¤©è¯·æ±‚
            request = ChatCompletionRequest(
                model=model,
                messages=messages,
                max_tokens=1000,
                temperature=0.7
            )
            
            # è·å–å®¢æˆ·ç«¯
            client = self.manager.get_client_by_model(model)
            start_time = asyncio.get_event_loop().time()
            response = await client.chat_completion(request)
            process_time = asyncio.get_event_loop().time() - start_time
            
            # è®°å½•å“åº”ä¿¡æ¯
            logger.info(f"\n{'='*50}\nğŸ¤– AI Response:\n"
                       f"Time: {process_time:.2f}s\n"
                       f"Content: {response.choices[0].message.content}\n"
                       f"Usage: {json.dumps(response.usage.__dict__, indent=2)}\n{'='*50}")
            
            return {
                "success": True,
                "user_message": message,
                "system_prompt": system_prompt,
                "model": model,
                "response": response.choices[0].message.content,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                    "completion_tokens": response.usage.completion_tokens if response.usage else 0,
                    "total_tokens": response.usage.total_tokens if response.usage else 0
                }
            }
            
        except Exception as e:
            logger.error(f"Chat completion failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "user_message": message,
                "system_prompt": system_prompt,
                "model": model
            }

    async def multi_turn_chat(
        self, 
        messages: List[Dict[str, str]], 
        model: str = "gemini-2.5-flash",
        system_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """å¤šè½®å¯¹è¯æ¥å£"""
        if not self._initialized:
            raise RuntimeError("DemoService not initialized")
        
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            chat_messages = []
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰systemæ¶ˆæ¯
            has_system_message = any(msg["role"] == "system" for msg in messages)
            
            # å¦‚æœæä¾›äº†system_promptä¸”æ¶ˆæ¯åˆ—è¡¨ä¸­æ²¡æœ‰systemæ¶ˆæ¯ï¼Œæ·»åŠ åˆ°å¼€å¤´
            if system_prompt and not has_system_message:
                chat_messages.append(Message(role=MessageRole.SYSTEM, content=system_prompt))
            
            # è½¬æ¢å…¶ä»–æ¶ˆæ¯
            for msg in messages:
                if msg["role"] == "user":
                    role = MessageRole.USER
                elif msg["role"] == "assistant":
                    role = MessageRole.ASSISTANT
                elif msg["role"] == "system":
                    role = MessageRole.SYSTEM
                else:
                    continue  # è·³è¿‡æœªçŸ¥è§’è‰²
                chat_messages.append(Message(role=role, content=msg["content"]))
            
            # è®°å½•è¯·æ±‚ä¿¡æ¯
            logger.info(f"\n{'='*50}\nğŸ¤– AI Request:\n"
                       f"Model: {model}\n"
                       f"Messages:\n{format_messages(chat_messages)}\n{'='*50}")
            
            # åˆ›å»ºèŠå¤©è¯·æ±‚
            request = ChatCompletionRequest(
                model=model,
                messages=chat_messages,
                max_tokens=1000,
                temperature=0.7
            )
            
            # ä½¿ç”¨ manager ç›´æ¥è·å–å®¢æˆ·ç«¯
            client = self.manager.get_client_by_model(model)
            start_time = asyncio.get_event_loop().time()
            response = await client.chat_completion(request)
            process_time = asyncio.get_event_loop().time() - start_time
            
            # è®°å½•å“åº”ä¿¡æ¯
            logger.info(f"\n{'='*50}\nğŸ¤– AI Response:\n"
                       f"Time: {process_time:.2f}s\n"
                       f"Content: {response.choices[0].message.content}\n"
                       f"Usage: {json.dumps(response.usage.__dict__, indent=2)}\n{'='*50}")
            
            return {
                "success": True,
                "conversation": messages,
                "system_prompt": system_prompt if not has_system_message else None,
                "model": model,
                "response": response.choices[0].message.content,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                    "completion_tokens": response.usage.completion_tokens if response.usage else 0,
                    "total_tokens": response.usage.total_tokens if response.usage else 0
                }
            }
            
        except Exception as e:
            logger.error(f"Multi-turn chat failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "conversation": messages,
                "system_prompt": system_prompt if not has_system_message else None,
                "model": model
            }

    async def stream_chat(
        self, 
        message: str, 
        model: str = "gemini-2.5-flash",
        system_prompt: Optional[str] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼èŠå¤©æ¥å£"""
        if not self._initialized:
            raise RuntimeError("DemoService not initialized")
        
        try:
            # å…ˆæ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
            available_models = self.manager.get_all_models()
            logger.info(f"Looking for model {model} in available models: {available_models}")
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = []
            
            # å¦‚æœæœ‰system promptï¼Œæ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨å¼€å¤´
            if system_prompt:
                messages.append(Message(role=MessageRole.SYSTEM, content=system_prompt))
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            messages.append(Message(role=MessageRole.USER, content=message))
            
            # è®°å½•è¯·æ±‚ä¿¡æ¯
            logger.info(f"\n{'='*50}\nğŸ¤– AI Stream Request:\n"
                       f"Model: {model}\n"
                       f"Messages:\n{format_messages(messages)}\n{'='*50}")
            
            # åˆ›å»ºèŠå¤©è¯·æ±‚
            request = ChatCompletionRequest(
                model=model,
                messages=messages,
                max_tokens=1000,
                temperature=0.7,
                stream=True
            )
            
            # è·å–å®¢æˆ·ç«¯å¹¶å¼€å§‹æµå¼è¯·æ±‚
            client = self.manager.get_client_by_model(model)
            
            # åˆå§‹åŒ–ç»Ÿè®¡
            total_content = ""
            chunk_count = 0
            start_time = asyncio.get_event_loop().time()
            
            # å‘é€å¼€å§‹äº‹ä»¶
            yield {
                "type": "start",
                "model": model,
                "user_message": message,
                "system_prompt": system_prompt,
                "timestamp": start_time
            }
            
            async for stream_response in client.chat_completion_stream(request):
                chunk_count += 1
                
                # è·å–å½“å‰chunkçš„å†…å®¹
                if stream_response.choices and len(stream_response.choices) > 0:
                    choice = stream_response.choices[0]
                    if choice.delta and choice.delta.content:
                        content_chunk = choice.delta.content
                        total_content += content_chunk
                        
                        yield {
                            "type": "content",
                            "content": content_chunk,
                            "chunk_index": chunk_count,
                            "model": model,
                            "timestamp": asyncio.get_event_loop().time()
                        }
                        
                        # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                        if choice.finish_reason:
                            yield {
                                "type": "finish",
                                "finish_reason": choice.finish_reason,
                                "total_content": total_content,
                                "total_chunks": chunk_count,
                                "model": model,
                                "timestamp": asyncio.get_event_loop().time()
                            }
                            break
            
            # å¦‚æœæ²¡æœ‰æ”¶åˆ°finishä¿¡å·ï¼Œå‘é€å®Œæˆäº‹ä»¶
            if chunk_count > 0:
                process_time = asyncio.get_event_loop().time() - start_time
                logger.info(f"\n{'='*50}\nğŸ¤– AI Stream Response:\n"
                           f"Time: {process_time:.2f}s\n"
                           f"Total Chunks: {chunk_count}\n"
                           f"Content: {total_content}\n{'='*50}")
                
                yield {
                    "type": "done",
                    "total_content": total_content,
                    "total_chunks": chunk_count,
                    "model": model,
                    "user_message": message,
                    "system_prompt": system_prompt,
                    "timestamp": asyncio.get_event_loop().time()
                }
            
        except Exception as e:
            logger.error(f"Stream chat failed: {e}")
            yield {
                "type": "error",
                "error": str(e),
                "user_message": message,
                "system_prompt": system_prompt,
                "model": model,
                "timestamp": asyncio.get_event_loop().time()
            }

    async def stream_multi_turn_chat(
        self, 
        messages: List[Dict[str, str]], 
        model: str = "gemini-2.5-flash",
        system_prompt: Optional[str] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼å¤šè½®å¯¹è¯æ¥å£"""
        if not self._initialized:
            raise RuntimeError("DemoService not initialized")
        
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            chat_messages = []
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰systemæ¶ˆæ¯
            has_system_message = any(msg["role"] == "system" for msg in messages)
            
            # å¦‚æœæä¾›äº†system_promptä¸”æ¶ˆæ¯åˆ—è¡¨ä¸­æ²¡æœ‰systemæ¶ˆæ¯ï¼Œæ·»åŠ åˆ°å¼€å¤´
            if system_prompt and not has_system_message:
                chat_messages.append(Message(role=MessageRole.SYSTEM, content=system_prompt))
            
            # è½¬æ¢å…¶ä»–æ¶ˆæ¯
            for msg in messages:
                if msg["role"] == "user":
                    role = MessageRole.USER
                elif msg["role"] == "assistant":
                    role = MessageRole.ASSISTANT
                elif msg["role"] == "system":
                    role = MessageRole.SYSTEM
                else:
                    continue  # è·³è¿‡æœªçŸ¥è§’è‰²
                chat_messages.append(Message(role=role, content=msg["content"]))
            
            # è®°å½•è¯·æ±‚ä¿¡æ¯
            logger.info(f"\n{'='*50}\nğŸ¤– AI Stream Request:\n"
                       f"Model: {model}\n"
                       f"Messages:\n{format_messages(chat_messages)}\n{'='*50}")
            
            # åˆ›å»ºèŠå¤©è¯·æ±‚
            request = ChatCompletionRequest(
                model=model,
                messages=chat_messages,
                max_tokens=1000,
                temperature=0.7,
                stream=True
            )
            
            # è·å–å®¢æˆ·ç«¯å¹¶å¼€å§‹æµå¼è¯·æ±‚
            client = self.manager.get_client_by_model(model)
            
            # åˆå§‹åŒ–ç»Ÿè®¡
            total_content = ""
            chunk_count = 0
            start_time = asyncio.get_event_loop().time()
            
            # å‘é€å¼€å§‹äº‹ä»¶
            yield {
                "type": "start",
                "model": model,
                "conversation": messages,
                "system_prompt": system_prompt if not has_system_message else None,
                "timestamp": start_time
            }
            
            async for stream_response in client.chat_completion_stream(request):
                chunk_count += 1
                
                # è·å–å½“å‰chunkçš„å†…å®¹
                if stream_response.choices and len(stream_response.choices) > 0:
                    choice = stream_response.choices[0]
                    if choice.delta and choice.delta.content:
                        content_chunk = choice.delta.content
                        total_content += content_chunk
                        
                        yield {
                            "type": "content",
                            "content": content_chunk,
                            "chunk_index": chunk_count,
                            "model": model,
                            "timestamp": asyncio.get_event_loop().time()
                        }
                        
                        # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                        if choice.finish_reason:
                            yield {
                                "type": "finish",
                                "finish_reason": choice.finish_reason,
                                "total_content": total_content,
                                "total_chunks": chunk_count,
                                "model": model,
                                "timestamp": asyncio.get_event_loop().time()
                            }
                            break
            
            # å¦‚æœæ²¡æœ‰æ”¶åˆ°finishä¿¡å·ï¼Œå‘é€å®Œæˆäº‹ä»¶
            if chunk_count > 0:
                process_time = asyncio.get_event_loop().time() - start_time
                logger.info(f"\n{'='*50}\nğŸ¤– AI Stream Response:\n"
                           f"Time: {process_time:.2f}s\n"
                           f"Total Chunks: {chunk_count}\n"
                           f"Content: {total_content}\n{'='*50}")
                
                yield {
                    "type": "done",
                    "total_content": total_content,
                    "total_chunks": chunk_count,
                    "model": model,
                    "conversation": messages,
                    "system_prompt": system_prompt if not has_system_message else None,
                    "timestamp": asyncio.get_event_loop().time()
                }
            
        except Exception as e:
            logger.error(f"Stream multi-turn chat failed: {e}")
            yield {
                "type": "error",
                "error": str(e),
                "conversation": messages,
                "system_prompt": system_prompt if not has_system_message else None,
                "model": model,
                "timestamp": asyncio.get_event_loop().time()
            }

# å…¨å±€å®ä¾‹
_demo_service = None

async def get_demo_service() -> DemoService:
    """
    è·å–DemoæœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    Returns:
        DemoServiceå®ä¾‹
    """
    global _demo_service
    if _demo_service is None:
        _demo_service = DemoService()
    return _demo_service

async def initialize_demo_service(gemini_config: Dict[str, Any]) -> None:
    """
    åˆå§‹åŒ–DemoæœåŠ¡
    
    Args:
        gemini_config: Geminié…ç½®
    """
    service = await get_demo_service()
    await service.initialize(gemini_config) 