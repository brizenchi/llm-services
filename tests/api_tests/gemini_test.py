#!/usr/bin/env python3
"""
Gemini API æµ‹è¯•è„šæœ¬

ä¸“é—¨æµ‹è¯•Google Gemini APIçš„è°ƒç”¨
"""

import asyncio
import os
import sys
import time
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥LLMç»„ä»¶
try:
    from pkg.core.llm import (
        GeminiProvider,
        Message,
        MessageRole,
        ChatCompletionRequest
    )
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    sys.exit(1)


async def test_gemini_basic():
    """æµ‹è¯•GeminiåŸºç¡€åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯• Gemini åŸºç¡€åŠŸèƒ½...")
    
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key or api_key.startswith("your-"):
        print("âš ï¸  è·³è¿‡ Gemini (æ— æœ‰æ•ˆAPIå¯†é’¥)")
        print("ğŸ’¡ è¯·åœ¨ deployment/.env ä¸­è®¾ç½® GEMINI_API_KEY")
        return False
    
    try:
        # åˆ›å»ºProvider
        provider = GeminiProvider.create_simple(
            api_key=api_key,
            model_name="gemini-2.0-flash-exp",
            base_url=os.getenv("GEMINI_BASE_URL", "https://generativelanguage.googleapis.com/v1beta")
        )
        
        await provider.initialize_all_models()
        client = provider.get_llm_client("gemini-2.0-flash-exp")
        
        # åˆ›å»ºè¯·æ±‚
        request = ChatCompletionRequest(
            model="gemini-2.0-flash-exp",
            messages=[
                Message(role=MessageRole.SYSTEM, content="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"),
                Message(role=MessageRole.USER, content="è¯·ç®€å•ä»‹ç»ä¸€ä¸‹Google Geminiæ¨¡å‹ã€‚")
            ],
            max_tokens=200,
            temperature=0.7
        )
        
        # æ‰§è¡Œè¯·æ±‚
        print(f"  ğŸ“¤ å‘é€æ¶ˆæ¯: è¯·ç®€å•ä»‹ç»ä¸€ä¸‹Google Geminiæ¨¡å‹")
        start_time = time.time()
        
        response = await client.chat_completion(request)
        
        duration = time.time() - start_time
        content = response.choices[0].message.content if response.choices else "æ— å“åº”"
        tokens = response.usage.total_tokens if response.usage else 0
        
        print(f"  âœ… Gemini å“åº” ({duration:.2f}s, {tokens} tokens):")
        print(f"     {content}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Gemini æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_gemini_models():
    """æµ‹è¯•ä¸åŒçš„Geminiæ¨¡å‹"""
    print("\nğŸ” æµ‹è¯•ä¸åŒçš„ Gemini æ¨¡å‹...")
    
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key or api_key.startswith("your-"):
        print("âš ï¸  è·³è¿‡æ¨¡å‹æµ‹è¯• (æ— æœ‰æ•ˆAPIå¯†é’¥)")
        return False
    
    # ä¸åŒçš„Geminiæ¨¡å‹
    models = [
        "gemini-2.5-flash",
    ]
    
    success_count = 0
    
    for model_name in models:
        try:
            print(f"\n  ğŸ“‹ æµ‹è¯•æ¨¡å‹: {model_name}")
            
            provider = GeminiProvider.create_simple(
                api_key=api_key,
                model_name=model_name
            )
            
            await provider.initialize_all_models()
            client = provider.get_llm_client(model_name)
            
            request = ChatCompletionRequest(
                model=model_name,
                messages=[
                    Message(role=MessageRole.USER, content="Hello! Please respond with 'Model test successful' in Chinese.")
                ],
                max_tokens=50
            )
            
            start_time = time.time()
            response = await client.chat_completion(request)
            duration = time.time() - start_time
            
            if response.choices:
                content = response.choices[0].message.content
                print(f"    âœ… {model_name}: {duration:.2f}s - {content[:50]}...")
                success_count += 1
            else:
                print(f"    âŒ {model_name}: æ— å“åº”")
                
        except Exception as e:
            print(f"    âŒ {model_name}: {str(e)[:100]}...")
    
    print(f"\nğŸ“Š æ¨¡å‹æµ‹è¯•ç»“æœ: {success_count}/{len(models)} æˆåŠŸ")
    return success_count > 0


async def test_gemini_stream():
    """æµ‹è¯•Geminiæµå¼å“åº”"""
    print("\nğŸŒŠ æµ‹è¯• Gemini æµå¼å“åº”...")
    
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key or api_key.startswith("your-"):
        print("âš ï¸  è·³è¿‡æµå¼æµ‹è¯• (æ— æœ‰æ•ˆAPIå¯†é’¥)")
        return False
    
    try:
        provider = GeminiProvider.create_simple(
            api_key=api_key,
            model_name="gemini-2.0-flash"
        )
        
        await provider.initialize_all_models()
        client = provider.get_llm_client("gemini-2.0-flash-exp")
        
        request = ChatCompletionRequest(
            model="gemini-2.0-flash-exp",
            messages=[
                Message(role=MessageRole.USER, content="è¯·ç”¨ä¸­æ–‡å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„çŸ­è¯—ï¼Œæ¯è¡Œå•ç‹¬è¾“å‡ºã€‚")
            ],
            max_tokens=200
        )
        
        print(f"  ğŸ“¤ å‘é€æµå¼è¯·æ±‚...")
        print(f"  ğŸ“ æµå¼å“åº”:")
        
        chunk_count = 0
        start_time = time.time()
        
        async for chunk in client.chat_completion_stream(request):
            if chunk.choices and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                print(f"     {content}", end="", flush=True)
                chunk_count += 1
        
        duration = time.time() - start_time
        print(f"\n  âœ… æµå¼æµ‹è¯•å®Œæˆ: {chunk_count} ä¸ªchunks, {duration:.2f}s")
        
        return True
        
    except Exception as e:
        print(f"  âŒ æµå¼æµ‹è¯•å¤±è´¥: {e}")
        return False


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª Google Gemini API æµ‹è¯•")
    print("=" * 50)
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    env_file = project_root / "deployment" / ".env"
    if env_file.exists():
        load_dotenv(env_file)
        print(f"ğŸ“‹ å·²åŠ è½½é…ç½®: {env_file}")
    else:
        print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {env_file}")
        print("ğŸ’¡ è¯·åˆ›å»ºé…ç½®æ–‡ä»¶å¹¶æ·»åŠ  GEMINI_API_KEY")
        return
    
    # æ£€æŸ¥APIå¯†é’¥
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("âŒ ç¼ºå°‘ GEMINI_API_KEY ç¯å¢ƒå˜é‡")
        print("ğŸ’¡ è¯·åœ¨ deployment/.env ä¸­æ·»åŠ :")
        print("   GEMINI_API_KEY=your-gemini-api-key-here")
        return
    
    if api_key.startswith("your-"):
        print("âŒ è¯·æ›¿æ¢ä¸ºçœŸå®çš„ Gemini API å¯†é’¥")
        return
    
    print(f"ğŸ”‘ ä½¿ç”¨APIå¯†é’¥: {api_key[:8]}...{api_key[-4:]}")
    print()
    
    # è¿è¡Œæµ‹è¯•
    results = []
    
    # åŸºç¡€åŠŸèƒ½æµ‹è¯•
    results.append(("åŸºç¡€åŠŸèƒ½", await test_gemini_basic()))
    
    # å¤šæ¨¡å‹æµ‹è¯•
    results.append(("æ¨¡å‹æµ‹è¯•", await test_gemini_models()))
    
    # æµå¼æµ‹è¯•
    results.append(("æµå¼å“åº”", await test_gemini_stream()))
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 50)
    
    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
    
    success_count = sum(1 for _, success in results if success)
    total_count = len(results)
    
    if success_count == total_count:
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼({success_count}/{total_count})")
    else:
        print(f"\nğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼š{success_count}/{total_count}")
        
    print("\nğŸ’¡ è·å–Gemini APIå¯†é’¥: https://ai.google.dev/")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc() 